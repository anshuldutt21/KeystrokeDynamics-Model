import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
import numpy as np
from tensorflow.keras.layers.experimental import preprocessing
from matplotlib import pyplot
from tensorflow import keras
from tensorflow.keras.layers import LeakyReLU
from keras.layers import Dropout                       #for random dropout
from tensorflow.keras import regularizers
from tensorflow.keras.layers import BatchNormalization
from sklearn.model_selection import KFold

dataset = pd.read_excel('/content/drive/MyDrive/DSL-StrongPasswordData.xls')
X = dataset.iloc[:,3:].values
y = dataset.iloc[:,0].values
# encode class values as integers
encoder = LabelEncoder()
encoder.fit(y)
encoded_Y = encoder.transform(y)
# convert integers to dummy variables (i.e. one hot encoded)
Y = np_utils.to_categorical(encoded_Y)

X = X.astype('float64')
# layer = preprocessing.Normalization()
# layer.adapt(X)
# X = layer(X)

X_test = X[200:400,:]
Y_test = Y[200:400,:]
X_train = X[0:180,:]
Y_train = Y[0:180,:]
X_val = X[180:200,:]
Y_val = Y[180:200,:]
X_final_train = X[0:200,:]
Y_final_train = Y[0:200,:]


for x in range(1,51):
  X_test = np.concatenate((X_test, X[200 + 400*x:400 + 400*x,:]), axis = 0)
  Y_test = np.concatenate((Y_test, Y[200 + 400*x:400 + 400*x,:]), axis = 0)
  
  X_train = np.concatenate((X_train, X[400*x:180 + 400*x,:]), axis = 0)
  Y_train = np.concatenate((Y_train, Y[400*x:180 + 400*x,:]), axis = 0)

  X_val = np.concatenate((X_val, X[180 + 400*x:200 + 400*x,:]), axis = 0)
  Y_val = np.concatenate((Y_val, Y[180 + 400*x:200 + 400*x,:]), axis = 0)

  X_final_train = np.concatenate((X_final_train, X[400*x:200 + 400*x,:]), axis = 0)
  Y_final_train = np.concatenate((Y_final_train, Y[400*x:200 + 400*x,:]), axis = 0)

layer = preprocessing.Normalization()
layer.adapt(X_final_train)
X_final_train = layer(X_final_train)

layer.adapt(X_test)
X_test = layer(X_test)

print(X_final_train.shape)

leaky_relu_alpha = 0.01
values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]
batch_size_vals = [32]
print("\n\n")
# Define per-fold score containers
acc_per_fold = []
loss_per_fold = []

# Define the K-fold Cross Validator
num_folds = 5
kfold = KFold(n_splits=num_folds, shuffle=True)

fold_no = 1
Xshape = X.shape
Yshape = Y.shape
initializer = keras.initializers.HeNormal()

for train, test in kfold.split(X, Y):
  Xtrain = np.empty(Xshape)
  Xtrain = Xtrain[0:0,:]
  
  Ytrain = np.empty(Yshape)
  Ytrain = Ytrain[0:0,:]
  
  Xtest = np.empty(Xshape)
  Xtest = Xtest[0:0,:]
  
  Ytest = np.empty(Yshape)
  Ytest = Ytest[0:0,:]

  for i in train:
    Xtrain = np.vstack([Xtrain, X[i,:]])
    Ytrain = np.vstack([Ytrain, Y[i,:]])

  for i in test:
    Xtest = np.vstack([Xtest, X[i,:]])
    Ytest = np.vstack([Ytest, Y[i,:]])

  layer = preprocessing.Normalization()
  layer.adapt(Xtrain)
  Xtrain = layer(Xtrain)

  layer.adapt(Xtest)
  Xtest = layer(Xtest)

  print(Xtrain)
  model = Sequential()
  model.add(Dense(512, kernel_initializer=initializer, bias_initializer='zeros', input_dim=31))
  model.add(LeakyReLU(alpha=leaky_relu_alpha))
  model.add(BatchNormalization())
  model.add(Dropout(0.30))

  model.add(Dense(256, kernel_initializer= initializer, bias_initializer='zeros'))
  model.add(LeakyReLU(alpha=leaky_relu_alpha))
  model.add(BatchNormalization())
  model.add(Dropout(0.50))

  model.add(Dense(144, kernel_initializer= initializer, bias_initializer='zeros'))
  model.add(LeakyReLU(alpha=leaky_relu_alpha))
  model.add(BatchNormalization())
  model.add(Dropout(0.30))

  model.add(Dense(51, activation='softmax'))
  model.summary()

  # Compile model
  optimizer = keras.optimizers.Adam(learning_rate=0.001)
  model.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics=['accuracy', keras.metrics.Precision(),keras.metrics.FalsePositives(), keras.metrics.FalseNegatives(), keras.metrics.Recall()])

  # Generate a print
  print('------------------------------------------------------------------------')
  print(f'Training for fold {fold_no} ...')

  # fit model
  history = model.fit(Xtrain, Ytrain, validation_data=(Xtest, Ytest), epochs=200, batch_size = 32)

  # evaluate the model
  # _, train_acc = model.evaluate(Xtrain, Ytrain, verbose=0)
  scores = model.evaluate(Xtest, Ytest, verbose=0)
  
  print(scores)
  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')
  acc_per_fold.append(scores[1] * 100)
  loss_per_fold.append(scores[0])

  fold_no = fold_no + 1

  # plot loss during training
  pyplot.subplot(211)
  pyplot.title('Loss')
  pyplot.plot(history.history['loss'], label='train')
  pyplot.plot(history.history['val_loss'], label='test')
  pyplot.legend()
  print("\n\n")
  # plot accuracy during training
  pyplot.subplot(212)
  pyplot.title('Accuracy')
  pyplot.plot(history.history['accuracy'], label='train')
  pyplot.plot(history.history['val_accuracy'], label='test')
  pyplot.legend()
  pyplot.show()

# == Provide average scores ==
print('------------------------------------------------------------------------')
print('Score per fold')
for i in range(0, len(acc_per_fold)):
  print('------------------------------------------------------------------------')
  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')
