{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN_Network_Keystroke_Dynamics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bmMzrqzZlAji3cdk-XN7lfVoAhdbynfu",
      "authorship_tag": "ABX9TyNOGYvLl2GHtuKL3ScetYw7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshuldutt21/KeystrokeDynamics-Model/blob/main/CNN_Network_Keystroke_Dynamics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiJtrTTQnmfY"
      },
      "source": [
        "# Convolutional Neural Network Model Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnJ3Qj7wntbE"
      },
      "source": [
        "This file experiments on multiple models of CNN to try and find out the best model. We have implemented VGG 16 model, Parallel block model and standard CNN model and compared them in our report. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3-gbeX3azNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "981d86d9-5f71-4d46-f08f-a5308b2b6273"
      },
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "dataset = pd.read_excel('/content/drive/MyDrive/DSL-StrongPasswordData.xls')\n",
        "X = dataset.iloc[:,3:].values\n",
        "y = dataset.iloc[:,0].values\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "encoded_Y = encoder.transform(y)\n",
        "\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "Y = np_utils.to_categorical(encoded_Y)\n",
        "\n",
        "X = X.astype('float64')\n",
        "\n",
        "# Train and test pre processed separately\n",
        "\n",
        "X_test = X[320:400,:]\n",
        "Y_test = Y[320:400,:]\n",
        "X_train = X[0:320,:]\n",
        "Y_train = Y[0:320,:]\n",
        "\n",
        "\n",
        "for x in range(1,51):\n",
        "  X_test = np.concatenate((X_test, X[320 + 400*x:400 + 400*x,:]), axis = 0)\n",
        "  Y_test = np.concatenate((Y_test, Y[320 + 400*x:400 + 400*x,:]), axis = 0)\n",
        "  \n",
        "  X_train = np.concatenate((X_train, X[400*x:320 + 400*x,:]), axis = 0)\n",
        "  Y_train = np.concatenate((Y_train, Y[400*x:320 + 400*x,:]), axis = 0)\n",
        "\n",
        "X_final_train = X_train\n",
        "Y_final_train = Y_train\n",
        "\n",
        "X_final_train, Y_final_train = shuffle(X_final_train, Y_final_train)\n",
        "layer = preprocessing.Normalization()\n",
        "layer.adapt(X_final_train)\n",
        "X_final_train = layer(X_final_train)\n",
        "\n",
        "layer.adapt(X_test)\n",
        "X_test = layer(X_test)\n",
        "\n",
        "print(X_final_train.shape)\n",
        "print(X_test.shape)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
            "(16320, 31)\n",
            "(4080, 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzweEJvypek-"
      },
      "source": [
        "**Parallel Block Layer CNN Model**: The key idea is to apply\n",
        "multiple rectangular kernels, instead of more typical square kernels. Specifically,\n",
        "the width of all kernels is the same as the embedding size for each word, so the\n",
        "output for each convolution is a one-dimension vector. Then multiple max-pooling\n",
        "layers are used to process these vectors to yield one feature for each kernel. Finally,\n",
        "these generated features are concatenated into a one-dimension vector, and multiple\n",
        "fully-connected layers are used to produce the class prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDq_Y-j6zxW8"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout                       #for random dropout\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from sklearn.model_selection import KFold\n",
        "from keras import Input, Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Concatenate, Activation, Dropout, Flatten, Dense\n",
        "\n",
        "#reshape data to fit model\n",
        "X_final_train = np.reshape(X_final_train, (16320,1,31,1))\n",
        "X_test = np.reshape(X_test, (4080,1,31,1))\n",
        "\n",
        "kernel_size= {}\n",
        "kernel_size[0]= [2,3,9]\n",
        "kernel_size[1]= [3,3,8]\n",
        "kernel_size[2]= [5,3,6]\n",
        "kernel_size[3]= [7,3,4]\n",
        "kernel_size[4]= [9,3,4]\n",
        "kernel_size[5]= [11,3,4]\n",
        "\n",
        "input_shape = (1,31,1)\n",
        "initializer = keras.initializers.HeNormal()\n",
        "\n",
        "inputs = Input(shape = (1,31,1))\n",
        "convs = []\n",
        "\n",
        "for k_no in range(len(kernel_size)):\n",
        "    conv = Conv2D(32, kernel_size = (kernel_size[k_no][0], kernel_size[k_no][1]), padding='same', activation='relu')(inputs)\n",
        "    pool = MaxPooling2D(pool_size=(2,2), padding = 'same')(conv)\n",
        "    for i in range(kernel_size[k_no][2]):\n",
        "      pool = MaxPooling2D(pool_size=(2,2), padding = 'same')(pool)\n",
        "    convs.append(pool)\n",
        "\n",
        "if len(kernel_size) > 1:\n",
        "    out = Concatenate()(convs)\n",
        "else:\n",
        "    out = convs[0]\n",
        "\n",
        "conv_model = Model(inputs, out)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(conv_model)\n",
        "model.add(Flatten(input_shape=input_shape))\n",
        "\n",
        "model.add(Dense(128, kernel_initializer=initializer, bias_initializer='zeros'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(256, kernel_initializer=initializer, bias_initializer='zeros'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(51, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics=['accuracy'])\n",
        "  \n",
        "# fit model\n",
        "history = model.fit(X_final_train, Y_final_train, validation_data=(X_test, Y_test), epochs=200, batch_size = 32)\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(X_final_train, Y_final_train, verbose=0)\n",
        "_, test_acc = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss during training\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Loss')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "print(\"\\n\\n\")\n",
        "# plot accuracy during training\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.plot(history.history['accuracy'], label='train')\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjSwFdHfqC92"
      },
      "source": [
        "**VGG - 16 Model:** It consists of 13 convolutional l layers with 3x3 filter size, 5 subsampling/ max pooling layer with size of 2x2, two fully connected layers with activation function and softmax function. Each block is made by consecutive 3 Ã— 3 convolutions and followed by a max pooling layer. The number of filter is gradually increased from size of 64,128,256,512 and 512. To avoid the problem of over-fitting, we use dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL8CDLjEFCsb"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout                       #for random dropout\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from sklearn.model_selection import KFold\n",
        "from keras import Input, Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Concatenate, Activation, Dropout, Flatten, Dense\n",
        "\n",
        "#reshape data to fit model\n",
        "X_final_train = np.reshape(X_final_train, (16320,1,31,1))\n",
        "X_test = np.reshape(X_test, (4080,1,31,1))\n",
        "\n",
        "input_shape = (1,31,1)\n",
        "\n",
        "inputs = Input(shape = (1,31,1))\n",
        "initializer = keras.initializers.HeNormal()\n",
        "\n",
        "conv1 = Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(inputs)\n",
        "conv1 = Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(conv1)\n",
        "conv1 = BatchNormalization()(conv1)\n",
        "max1 = MaxPooling2D(pool_size=(2,2), padding = 'same')(conv1)\n",
        "\n",
        "conv2 = Conv2D(128, (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(max1)\n",
        "conv2 = Conv2D(128, (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(conv2)\n",
        "conv2 = BatchNormalization()(conv2)\n",
        "max2 = MaxPooling2D(pool_size=(2,2), padding = 'same')(conv2)\n",
        "\n",
        "conv3 = Conv2D(256, (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(max2)\n",
        "conv3 = Conv2D(256, (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(conv3)\n",
        "conv3 = Conv2D(256, (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(conv3)\n",
        "conv3 = BatchNormalization()(conv3)\n",
        "max3 = MaxPooling2D(pool_size=(2,2), padding = 'same')(conv3)\n",
        "\n",
        "conv4 = Conv2D(512, (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(max3)\n",
        "conv4 = Conv2D(512, (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(conv4)\n",
        "conv4 = Conv2D(512, (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(conv4)\n",
        "conv4 = BatchNormalization()(conv4)\n",
        "max4 = MaxPooling2D(pool_size=(2,2), padding = 'same')(conv4)\n",
        "\n",
        "conv5 = Conv2D(512, (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(max4)\n",
        "conv5 = Conv2D(512, (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(conv4)\n",
        "conv5 = Conv2D(512, (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(conv4)\n",
        "conv5 = BatchNormalization()(conv5)\n",
        "max5 = MaxPooling2D(pool_size=(2,2), padding = 'same')(conv5)\n",
        "\n",
        "flat = Flatten()(max5)\n",
        "\n",
        "den1 = Dense(512, activation = 'relu')(flat)\n",
        "den1 = Dropout(0.5)(den1)\n",
        "out1 = Dense(51, activation ='softmax')(den1)\n",
        "\n",
        "model = Model(inputs, out1)\n",
        "\n",
        "# Compile model\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics=['accuracy', keras.metrics.Precision(),keras.metrics.FalsePositives(), keras.metrics.FalseNegatives(), keras.metrics.Recall()])\n",
        "  \n",
        "# fit model\n",
        "history = model.fit(X_final_train, Y_final_train, validation_data=(X_test, Y_test), epochs=200, batch_size = 32)\n",
        "\n",
        "# evaluate the model\n",
        "_, test_acc = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss during training\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Loss')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "# plot accuracy during training\n",
        "print(\"\\n\\n\")\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.plot(history.history['accuracy'], label='train')\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqL9Jld6sAFf"
      },
      "source": [
        "**Standard CNN Network:** The CNN model consists of four hidden layers (three convolutional sub sampling layer pair and one fully connected layer), one input layer and one output layer.  The convolutional- sub sampling layer use the size 3x3 stride lengths followed by 2x2 regions. The number of filter is gradually increased from size of 64,128 and 256. The fully connected layer contains 1024 neurons with Rectified Linear Unit(ReLU) as activation funciton."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CE99mcIzkZ8w"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout                       #for random dropout\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from sklearn.model_selection import KFold\n",
        "from keras import Input, Sequential\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D, MaxPooling2D, Concatenate, Activation, Dropout, Flatten, Dense\n",
        "\n",
        "\n",
        "input_shape = (1,31,1)\n",
        "\n",
        "inputs = Input(shape = (1,31,1))\n",
        "initializer = keras.initializers.HeNormal()\n",
        "\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "num_folds = 5\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "fold_no = 1\n",
        "Xshape = X.shape\n",
        "Yshape = Y.shape\n",
        "\n",
        "for train, test in kfold.split(X, Y):\n",
        "  Xtrain = np.empty(Xshape)\n",
        "  Xtrain = Xtrain[0:0,:]\n",
        "  \n",
        "  Ytrain = np.empty(Yshape)\n",
        "  Ytrain = Ytrain[0:0,:]\n",
        "  \n",
        "  Xtest = np.empty(Xshape)\n",
        "  Xtest = Xtest[0:0,:]\n",
        "  \n",
        "  Ytest = np.empty(Yshape)\n",
        "  Ytest = Ytest[0:0,:]\n",
        "\n",
        "  for i in train:\n",
        "    Xtrain = np.vstack([Xtrain, X[i,:]])\n",
        "    Ytrain = np.vstack([Ytrain, Y[i,:]])\n",
        "\n",
        "  for i in test:\n",
        "    Xtest = np.vstack([Xtest, X[i,:]])\n",
        "    Ytest = np.vstack([Ytest, Y[i,:]])\n",
        "\n",
        "  print(Xtrain)\n",
        "  layer = preprocessing.Normalization()\n",
        "  layer.adapt(Xtrain)\n",
        "  Xtrain = layer(Xtrain)\n",
        "  layer.adapt(Xtest)\n",
        "  Xtest = layer(Xtest)\n",
        "  \n",
        "  Xtrain = np.reshape(Xtrain, (16320,1,31,1))\n",
        "  Xtest = np.reshape(Xtest, (4080,1,31,1))\n",
        "\n",
        "\n",
        "  conv1 = Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(inputs)\n",
        "  conv1 = BatchNormalization()(conv1)\n",
        "  conv1 = Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(conv1)\n",
        "  conv1 = BatchNormalization()(conv1)\n",
        "  conv1 = Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(conv1)\n",
        "  conv1 = BatchNormalization()(conv1)\n",
        "  max1 = MaxPooling2D(pool_size=(2,2), padding = 'same')(conv1)\n",
        "\n",
        "  flat = Flatten()(max1)\n",
        "\n",
        "  den1 = Dense(512, activation = 'relu')(flat)\n",
        "  den1 = BatchNormalization()(den1)\n",
        "  den1 = Dropout(0.5)(den1)\n",
        "  out1 = Dense(51, activation ='softmax')(den1)\n",
        "\n",
        "  model = Model(inputs, out1)\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Compile model\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics=['accuracy', keras.metrics.Precision(),keras.metrics.FalsePositives(), keras.metrics.FalseNegatives(), keras.metrics.Recall()])\n",
        "    \n",
        "  # fit model\n",
        "  history = model.fit(Xtrain, Ytrain, validation_data=(Xtest, Ytest), epochs=200, batch_size = 32)\n",
        "\n",
        "  # evaluate the model\n",
        "  scores = model.evaluate(Xtest, Ytest, verbose=0)\n",
        "  \n",
        "  print(scores)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # plot loss during training\n",
        "  pyplot.subplot(211)\n",
        "  pyplot.title('Loss')\n",
        "  pyplot.plot(history.history['loss'], label='train')\n",
        "  pyplot.plot(history.history['val_loss'], label='test')\n",
        "  pyplot.legend()\n",
        "  print(\"\\n\\n\")\n",
        "  # plot accuracy during training\n",
        "  pyplot.subplot(212)\n",
        "  pyplot.title('Accuracy')\n",
        "  pyplot.plot(history.history['accuracy'], label='train')\n",
        "  pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "  pyplot.legend()\n",
        "  pyplot.show()\n",
        "\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jbKCl9bPHJQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}