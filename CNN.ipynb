import pandas as pd
import random
from keras.models import Sequential
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
import numpy as np
from sklearn.utils import shuffle
from tensorflow.keras.layers.experimental import preprocessing
from matplotlib import pyplot
from tensorflow import keras
from tensorflow.keras.layers import LeakyReLU
from keras.layers import Dropout                       #for random dropout
from tensorflow.keras import regularizers
from tensorflow.keras.layers import BatchNormalization
from sklearn.model_selection import KFold
from keras import Input, Sequential
from keras.models import Model
from keras.layers import Conv2D, MaxPooling2D, Concatenate, Activation, Dropout, Flatten, Dense

dataset = pd.read_excel('/content/drive/MyDrive/DSL-StrongPasswordData.xls')
X = dataset.iloc[:,3:].values
y = dataset.iloc[:,0].values
# encode class values as integers
encoder = LabelEncoder()
encoder.fit(y)
encoded_Y = encoder.transform(y)
# convert integers to dummy variables (i.e. one hot encoded)
Y = np_utils.to_categorical(encoded_Y)

X = X.astype('float64')

# Train and test pre processed separately
# layer = preprocessing.Normalization()
# layer.adapt(X)
# X = layer(X)

X_test = X[320:400,:]
Y_test = Y[320:400,:]
X_train = X[0:320,:]
Y_train = Y[0:320,:]


for x in range(1,51):
  X_test = np.concatenate((X_test, X[320 + 400*x:400 + 400*x,:]), axis = 0)
  Y_test = np.concatenate((Y_test, Y[320 + 400*x:400 + 400*x,:]), axis = 0)
  
  X_train = np.concatenate((X_train, X[400*x:320 + 400*x,:]), axis = 0)
  Y_train = np.concatenate((Y_train, Y[400*x:320 + 400*x,:]), axis = 0)

X_final_train = X_train
Y_final_train = Y_train

# for train in range(len(X_train)):
#   augmented_array = np.empty([2,31])

#   for i in range(31):
#     augmented_array[0, i] = random.randrange(-200,200)/10000
#     augmented_array[1, i] = random.randrange(-200,200)/10000
  
#   X_augmented_data = X_train[train,:] + augmented_array

#   X_final_train = np.vstack([X_final_train, X_augmented_data[0,:]])
#   Y_final_train = np.vstack([Y_final_train, Y[train,:]])
#   X_final_train = np.vstack([X_final_train, X_augmented_data[1,:]])
#   Y_final_train = np.vstack([Y_final_train, Y[train,:]])


X_final_train, Y_final_train = shuffle(X_final_train, Y_final_train)
layer = preprocessing.Normalization()
layer.adapt(X_final_train)
X_final_train = layer(X_final_train)

layer.adapt(X_test)
X_test = layer(X_test)

print(X_final_train.shape)
print(X_test.shape)

input_shape = (1,31,1)

inputs = Input(shape = (1,31,1))
initializer = keras.initializers.HeNormal()

# Define per-fold score containers
acc_per_fold = []
loss_per_fold = []

# Define the K-fold Cross Validator
num_folds = 5
kfold = KFold(n_splits=num_folds, shuffle=True)

fold_no = 1
Xshape = X.shape
Yshape = Y.shape

for train, test in kfold.split(X, Y):
  Xtrain = np.empty(Xshape)
  Xtrain = Xtrain[0:0,:]
  
  Ytrain = np.empty(Yshape)
  Ytrain = Ytrain[0:0,:]
  
  Xtest = np.empty(Xshape)
  Xtest = Xtest[0:0,:]
  
  Ytest = np.empty(Yshape)
  Ytest = Ytest[0:0,:]

  for i in train:
    Xtrain = np.vstack([Xtrain, X[i,:]])
    Ytrain = np.vstack([Ytrain, Y[i,:]])

  for i in test:
    Xtest = np.vstack([Xtest, X[i,:]])
    Ytest = np.vstack([Ytest, Y[i,:]])

  print(Xtrain)
  layer = preprocessing.Normalization()
  layer.adapt(Xtrain)
  Xtrain = layer(Xtrain)
  layer.adapt(Xtest)
  Xtest = layer(Xtest)
  
  Xtrain = np.reshape(Xtrain, (16320,1,31,1))
  Xtest = np.reshape(Xtest, (4080,1,31,1))


  conv1 = Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(inputs)
  # conv1 = BatchNormalization()(conv1)
  conv1 = Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(conv1)
  # conv1 = BatchNormalization()(conv1)
  conv1 = Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same', activation = 'relu', kernel_initializer=initializer)(conv1)
  # conv1 = BatchNormalization()(conv1)
  max1 = MaxPooling2D(pool_size=(2,2), padding = 'same')(conv1)

  flat = Flatten()(max1)

  den1 = Dense(512, activation = 'relu')(flat)
  den1 = BatchNormalization()(den1)
  den1 = Dropout(0.5)(den1)
  out1 = Dense(51, activation ='softmax')(den1)

  model = Model(inputs, out1)

  # Generate a print
  print('------------------------------------------------------------------------')
  print(f'Training for fold {fold_no} ...')

  # Compile model
  optimizer = keras.optimizers.Adam(learning_rate=0.0001)
  model.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics=['accuracy', keras.metrics.Precision(),keras.metrics.FalsePositives(), keras.metrics.FalseNegatives(), keras.metrics.Recall()])
    
  # fit model
  history = model.fit(Xtrain, Ytrain, validation_data=(Xtest, Ytest), epochs=200, batch_size = 32)

  # evaluate the model
  # _, train_acc = model.evaluate(Xtrain, Ytrain, verbose=0)
  scores = model.evaluate(Xtest, Ytest, verbose=0)
  
  print(scores)
  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')
  acc_per_fold.append(scores[1] * 100)
  loss_per_fold.append(scores[0])

  # plot loss during training
  pyplot.subplot(211)
  pyplot.title('Loss')
  pyplot.plot(history.history['loss'], label='train')
  pyplot.plot(history.history['val_loss'], label='test')
  pyplot.legend()
  print("\n\n")
  # plot accuracy during training
  pyplot.subplot(212)
  pyplot.title('Accuracy')
  pyplot.plot(history.history['accuracy'], label='train')
  pyplot.plot(history.history['val_accuracy'], label='test')
  pyplot.legend()
  pyplot.show()

  fold_no = fold_no + 1

# == Provide average scores ==
print('------------------------------------------------------------------------')
print('Score per fold')
for i in range(0, len(acc_per_fold)):
  print('------------------------------------------------------------------------')
  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')
